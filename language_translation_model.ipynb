{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tikendraw/language-translation-model/blob/main/language_translation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1hJ48ytxM3b"
   },
   "source": [
    "# Language Translation Model (English to Hindi)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrmDJdXC9SfD",
    "outputId": "3f4f9508-79e0-4a16-e63c-31240521c12f"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "    ! git clone https://github.com/tikendraw/language-translation-model.git \n",
    "    os.chdir('language-translation-model') \n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cxy3JvX9Fw4",
    "outputId": "ba911dec-56ea-4f65-f457-5ef48e944fe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 12:01:42.813144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 12:01:44.274116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/t/miniconda3/envs/tf_new/lib/\n",
      "2023-02-20 12:01:44.274217: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/t/miniconda3/envs/tf_new/lib/\n",
      "2023-02-20 12:01:44.274226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf version:  2.11.0\n",
      "GPU:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 12:01:46.901052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:46.914178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:46.914381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "# ! pip install polars -q\n",
    "import polars as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model , optimizers\n",
    "from tensorflow.keras.layers import Attention, LSTM, Bidirectional, Dense, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, AveragePooling1D, Dropout, concatenate, Concatenate\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install tensorflow_hub\n",
    "# import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Tf version: ',tf.__version__)\n",
    "print('GPU: ', is_gpu:=len(tf.config.list_physical_devices('GPU')))\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = './tmp/tfhub'\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "None\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "if is_gpu:\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(physical_devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nY49Vs8mxGcD"
   },
   "outputs": [],
   "source": [
    "dataset_url = 'http://www.manythings.org/anki/hin-eng.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7P06oG8xiS7",
    "outputId": "e30407c7-7313-45f6-ab97-1f3f2e45156d"
   },
   "outputs": [],
   "source": [
    "# # Download the dataset\n",
    "if 'google.colab' in sys.modules:\n",
    "    # donwload\n",
    "    !wget $dataset_url -P dataset\n",
    "\n",
    "    # # Unzip the downloaded file\n",
    "    !unzip ./dataset/hin-eng.zip -d ./dataset\n",
    "\n",
    "    # # Show size\n",
    "    !du -h  ./dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITptJktjzLho"
   },
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XP6M5Eb0t5F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vraPNhwl0t2Z"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/hin.txt', sep = '\\t', new_columns = ['english', 'hindi', 'somethingelse'])[['english','hindi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsHrOgJX0t0m",
    "outputId": "fd37ee3b-6231-4771-c8a2-62eb3d3e58b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "_iYNVAyD0tyx",
    "outputId": "67110dfe-9123-4b0a-b481-6f169a1a2493"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        white-space: pre;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-top: 0;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-bottom: 0;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        line-height: 95%;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "<small>shape: (10, 2)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "english\n",
       "</th>\n",
       "<th>\n",
       "hindi\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "str\n",
       "</td>\n",
       "<td>\n",
       "str\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;She is dressed...\n",
       "</td>\n",
       "<td>\n",
       "&quot;उसने सफ़ेद कपड...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;That&#x27;s his spe...\n",
       "</td>\n",
       "<td>\n",
       "&quot;वह उसकी ख़ासिय...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;We put sugar i...\n",
       "</td>\n",
       "<td>\n",
       "&quot;हम चाय में चीन...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;Turn your face...\n",
       "</td>\n",
       "<td>\n",
       "&quot;अपना मूँह इस ओ...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;I know that al...\n",
       "</td>\n",
       "<td>\n",
       "&quot;मैं जानता हूँ ...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;You can&#x27;t prov...\n",
       "</td>\n",
       "<td>\n",
       "&quot;तुम वह साबित न...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;What movies ar...\n",
       "</td>\n",
       "<td>\n",
       "&quot;इस सप्ताह कौन ...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;Everyone knew ...\n",
       "</td>\n",
       "<td>\n",
       "&quot;वह गाना सभी को...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;He wiped the s...\n",
       "</td>\n",
       "<td>\n",
       "&quot;उसने अपने माथे...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "&quot;We stayed with...\n",
       "</td>\n",
       "<td>\n",
       "&quot;हमने पूरी गर्म...\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "┌─────────────────────────────────────┬───────────────────────────────┐\n",
       "│ english                             ┆ hindi                         │\n",
       "│ ---                                 ┆ ---                           │\n",
       "│ str                                 ┆ str                           │\n",
       "╞═════════════════════════════════════╪═══════════════════════════════╡\n",
       "│ She is dressed in white.            ┆ उसने सफ़ेद कपड़े पहने हुएँ हैं।         │\n",
       "│ That's his specialty.               ┆ वह उसकी ख़ासियत है।             │\n",
       "│ We put sugar in our tea.            ┆ हम चाय में चीनी डालते हैं।         │\n",
       "│ Turn your face this way.            ┆ अपना मूँह इस ओर मोड़ो।           │\n",
       "│ ...                                 ┆ ...                           │\n",
       "│ What movies are playing this wee... ┆ इस सप्ताह कौन सी फ़िल्में लगीं ह... │\n",
       "│ Everyone knew the song.             ┆ वह गाना सभी को पता था।        │\n",
       "│ He wiped the sweat from his fore... ┆ उसने अपने माथे से पसीना पोंछा।     │\n",
       "│ We stayed with them all through ... ┆ हमने पूरी गर्मी उनके साथ बिताई।   │\n",
       "└─────────────────────────────────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "swlPFpr-0txP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32\n",
    "EMBEDDING_DIMS = 16\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceSkRUmM0twM"
   },
   "source": [
    "# Prepare the data `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BCw6ZgqV0tuW"
   },
   "outputs": [],
   "source": [
    "# Split the data for train and val\n",
    "train_df, val_df = train_test_split(df, test_size = .02, random_state = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kA0bZi7Z0ttP",
    "outputId": "a8d5a1a7-22b7-4d5f-e981-d24d92cacb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (2849, 2)\n",
      "val shape:  (59, 2)\n"
     ]
    }
   ],
   "source": [
    "print('train shape: ', train_df.shape)\n",
    "print('val shape: ', val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "puSucMUn0tq-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 12:01:47.008042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:47.008452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:47.008669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:48.040352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:48.040586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:48.040734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-20 12:01:48.040846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2423 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['english'].to_list(), train_df['hindi'].to_list())).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_df['english'].to_list(), val_df['hindi'].to_list())).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_APB0Ah64b26"
   },
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing text\n",
    "def tf_lower_and_split_punct_en(text):\n",
    "    # Split accented characters.\n",
    "    # text = tf.text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing text\n",
    "def tf_lower_and_split_punct_hi(text):\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfNyTkJ56pPt",
    "outputId": "198f22c5-740f-40dc-e3da-d1d7aa571a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "उन्होंने मेरी का| मज़ाक उड़ाया\n",
      "tf.Tensor(b'[START] \\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\x95\\xe0\\xa4\\xbe |  \\xe0\\xa4\\xae\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\x95 \\xe0\\xa4\\x89\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe [END]', shape=(), dtype=string)\n",
      "[START] उन्होंने मेरी का |  मज़ाक उड़ाया [END]\n"
     ]
    }
   ],
   "source": [
    "some_hindi_text = 'उन्होंने मेरी का| मज़ाक उड़ाया'\n",
    "print(some_hindi_text)\n",
    "b= tf_lower_and_split_punct_hi(some_hindi_text)\n",
    "print(b)\n",
    "print(b.numpy().decode())\n",
    "del(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRZEds9s0tpU",
    "outputId": "9b81f547-c419-483f-f275-e001812874f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  tf.Tensor(\n",
      "[b'The boy gathered a handful of peanuts and put them in a small box.'\n",
      " b'A madman is not accountable for his actions.' b'Tom is my friend.'\n",
      " b'Some of the apples in the box were rotten.' b'Everybody likes her.'\n",
      " b'She showed me a letter written in English.' b'Boys will be boys.'\n",
      " b'How rude of you!' b'What put such an idea into your head?'\n",
      " b'How old might his grandfather be?' b'How are you?'\n",
      " b'Bern is the capital of Switzerland.'\n",
      " b\"I learned to drive a car and got a driver's license when I was eighteen.\"\n",
      " b\"When he arrives in Tokyo, I'll call you right away.\"\n",
      " b\"We'll cross the river in a boat.\"\n",
      " b\"It's up to you whether to buy it or not.\" b'I am grateful to them.'\n",
      " b'He explained his plans in detail.' b'I explained the rule to him.'\n",
      " b\"That's Tom's house with the red roof.\" b'The reason is very simple.'\n",
      " b'Come with us.' b'Birds fly.'\n",
      " b'They were about to leave when I arrived there.'\n",
      " b'When are you going to Europe?' b'Please throw the ball.'\n",
      " b\"He's always running short of cash.\" b\"That's no business of yours.\"\n",
      " b'She is not afraid of anything.' b\"It's windy, isn't it?\"\n",
      " b'I have just cleaned my room.'\n",
      " b'There are lots of things for us to think about.'], shape=(32,), dtype=string)\n",
      "j:  tf.Tensor(\n",
      "[b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8 \\xe0\\xa4\\xb2\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x8f\\xe0\\xa4\\x95 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9f\\xe0\\xa5\\x8d\\xe0\\xa4\\xa0\\xe0\\xa5\\x80\\xe0\\xa4\\xad\\xe0\\xa4\\xb0 \\xe0\\xa4\\xae\\xe0\\xa5\\x82\\xe0\\xa4\\x81\\xe0\\xa4\\x97\\xe0\\xa4\\xab\\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa4\\x81 \\xe0\\xa4\\xb2\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\x8f\\xe0\\xa4\\x95 \\xe0\\xa4\\x9b\\xe0\\xa5\\x8b\\xe0\\xa4\\x9f\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xa1\\xe0\\xa4\\xac\\xe0\\xa5\\x8d\\xe0\\xa4\\xac\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xa1\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x8f\\xe0\\xa4\\x95 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\x97\\xe0\\xa4\\xb2 \\xe0\\xa4\\x87\\xe0\\xa4\\xa8\\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8 \\xe0\\xa4\\x95\\xe0\\xa5\\x8b \\xe0\\xa4\\x85\\xe0\\xa4\\xaa\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa5\\x87-\\xe0\\xa4\\xa7\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f \\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbf\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xa6\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xa0\\xe0\\xa4\\xb9\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe \\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x9f\\xe0\\xa5\\x89\\xe0\\xa4\\xae \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe \\xe0\\xa4\\xa6\\xe0\\xa5\\x8b\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa4 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xa1\\xe0\\xa4\\xac\\xe0\\xa5\\x8d\\xe0\\xa4\\xac\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa5\\x81\\xe0\\xa4\\x9b \\xe0\\xa4\\xb8\\xe0\\xa5\\x87\\xe0\\xa4\\xb5 \\xe0\\xa4\\xb8\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\x8f \\xe0\\xa4\\xa5\\xe0\\xa5\\x87\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa4\\xac \\xe0\\xa4\\xaa\\xe0\\xa4\\xb8\\xe0\\xa4\\x82\\xe0\\xa4\\xa6 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa5\\x87 \\xe0\\xa4\\x85\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa5\\x87\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa5\\x80 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x96\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\x88 \\xe0\\xa4\\x9a\\xe0\\xa4\\xbf\\xe0\\xa4\\x9f\\xe0\\xa5\\x8d\\xe0\\xa4\\xa0\\xe0\\xa5\\x80 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\x96\\xe0\\xa4\\xbe\\xe0\\xa4\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb2\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xa4\\xe0\\xa5\\x8b \\xe0\\xa4\\xb2\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x80 \\xe0\\xa4\\xb0\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa5\\x87\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xa4\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xac\\xe0\\xa4\\xa6\\xe0\\xa4\\xa4\\xe0\\xa4\\xae\\xe0\\xa5\\x80\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae!'\n",
      " b'\\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\xae\\xe0\\xa4\\xbe\\xe0\\xa4\\x97\\xe0\\xa4\\xbc \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\xac\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4 \\xe0\\xa4\\x95\\xe0\\xa5\\x88\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x86\\xe0\\xa4\\x88?'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\x89\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\x9a\\xe0\\xa4\\xbe\\xe0\\xa4\\xb9\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f?'\n",
      " b'\\xe0\\xa4\\xa4\\xe0\\xa5\\x82 \\xe0\\xa4\\x95\\xe0\\xa5\\x88\\xe0\\xa4\\xb8\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88?'\n",
      " b'\\xe0\\xa4\\xac\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa8 \\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb5\\xe0\\xa4\\xbf\\xe0\\xa4\\x9c\\xe0\\xa4\\xb0\\xe0\\xa4\\xb2\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa4\\xa1 \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\xb0\\xe0\\xa4\\xbe\\xe0\\xa4\\x9c\\xe0\\xa4\\xa7\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x9c\\xe0\\xa4\\xac \\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\x85\\xe0\\xa4\\xa0\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa4\\xb9 \\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2 \\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\xa5\\xe0\\xa4\\xbe \\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x97\\xe0\\xa4\\xbe\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa5\\x80 \\xe0\\xa4\\x9a\\xe0\\xa4\\xb2\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\xb8\\xe0\\xa5\\x80\\xe0\\xa4\\x96\\xe0\\xa5\\x80 \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbe\\xe0\\xa4\\x88\\xe0\\xa4\\xb8\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\xb8 \\xe0\\xa4\\xac\\xe0\\xa4\\xa8\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\x9f\\xe0\\xa5\\x8b\\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa5\\x8b \\xe0\\xa4\\x86\\xe0\\xa4\\xa4\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x80 \\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xb0\\xe0\\xa4\\x82\\xe0\\xa4\\xa4 \\xe0\\xa4\\xab\\xe0\\xa4\\xbc\\xe0\\xa5\\x8b\\xe0\\xa4\\xa8 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa5\\x82\\xe0\\xa4\\x81\\xe0\\xa4\\x97\\xe0\\xa5\\x80\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae \\xe0\\xa4\\xa8\\xe0\\xa4\\xbe\\xe0\\xa4\\xb5 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xa8\\xe0\\xa4\\xa6\\xe0\\xa5\\x80 \\xe0\\xa4\\x95\\xe0\\xa5\\x8b \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa5\\x87\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x96\\xe0\\xa4\\xbc\\xe0\\xa4\\xb0\\xe0\\xa5\\x80\\xe0\\xa4\\xa6\\xe0\\xa5\\x8b \\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8 \\xe0\\xa4\\x96\\xe0\\xa4\\xbc\\xe0\\xa4\\xb0\\xe0\\xa5\\x80\\xe0\\xa4\\xa6\\xe0\\xa5\\x8b \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\x8a\\xe0\\xa4\\xaa\\xe0\\xa4\\xb0 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\x86\\xe0\\xa4\\xad\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x82\\xe0\\xa4\\x81\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xb5\\xe0\\xa4\\xbf\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa4\\xe0\\xa5\\x83\\xe0\\xa4\\xa4 \\xe0\\xa4\\xb0\\xe0\\xa5\\x82\\xe0\\xa4\\xaa \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x85\\xe0\\xa4\\xaa\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\xaf\\xe0\\xa5\\x8b\\xe0\\xa4\\x9c\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\xb8\\xe0\\xa4\\xae\\xe0\\xa4\\x9d\\xe0\\xa4\\xbe\\xe0\\xa4\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xae \\xe0\\xa4\\xb8\\xe0\\xa4\\xae\\xe0\\xa4\\x9d\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb2\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2 \\xe0\\xa4\\x9b\\xe0\\xa4\\xa4 \\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa4\\xbe \\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\x9f\\xe0\\xa5\\x89\\xe0\\xa4\\xae \\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\x98\\xe0\\xa4\\xb0 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x95\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa4\\xa3 \\xe0\\xa4\\xac\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\xa4 \\xe0\\xa4\\xb8\\xe0\\xa4\\xb0\\xe0\\xa4\\xb2 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xa5 \\xe0\\xa4\\x86\\xe0\\xa4\\x93\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xaa\\xe0\\xa4\\x82\\xe0\\xa4\\x9b\\xe0\\xa5\\x80 \\xe0\\xa4\\x89\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\xa4\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x9c\\xe0\\xa4\\xac \\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\xb5\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe\\xe0\\xa4\\x81 \\xe0\\xa4\\xaa\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\x81\\xe0\\xa4\\x9a\\xe0\\xa4\\xbe, \\xe0\\xa4\\xb5\\xe0\\xa5\\x87 \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa5\\x87 \\xe0\\xa4\\xa5\\xe0\\xa5\\x87\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae \\xe0\\xa4\\xaf\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x8b\\xe0\\xa4\\xaa \\xe0\\xa4\\x95\\xe0\\xa4\\xac \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe \\xe0\\xa4\\xb0\\xe0\\xa4\\xb9\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b?'\n",
      " b'\\xe0\\xa4\\x97\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\xa6 \\xe0\\xa4\\xab\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa5\\x8b \\xe0\\xa4\\xb9\\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xb6\\xe0\\xa4\\xbe \\xe0\\xa4\\xaa\\xe0\\xa5\\x88\\xe0\\xa4\\xb8\\xe0\\xa5\\x8b\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\x95\\xe0\\xa4\\xae\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\x87\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x8b\\xe0\\xa4\\x88 \\xe0\\xa4\\xb2\\xe0\\xa5\\x87\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe-\\xe0\\xa4\\xa6\\xe0\\xa5\\x87\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xb8\\xe0\\xa5\\x80 \\xe0\\xa4\\xad\\xe0\\xa5\\x80 \\xe0\\xa4\\x9a\\xe0\\xa5\\x80\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xa1\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xaf\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe\\xe0\\xa4\\x81 \\xe0\\xa4\\xb9\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe \\xe0\\xa4\\x9a\\xe0\\xa4\\xb2 \\xe0\\xa4\\xb0\\xe0\\xa4\\xb9\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88, \\xe0\\xa4\\xb9\\xe0\\xa5\\x88 \\xe0\\xa4\\xa8\\xe0\\xa4\\xbe?'\n",
      " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x85\\xe0\\xa4\\xaa\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x95\\xe0\\xa4\\xae\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe \\xe0\\xa4\\x85\\xe0\\xa4\\xad\\xe0\\xa5\\x80 \\xe0\\xa4\\x85\\xe0\\xa4\\xad\\xe0\\xa5\\x80 \\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xab\\xe0\\xa4\\xbc \\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
      " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb8 \\xe0\\xa4\\xb8\\xe0\\xa5\\x8b\\xe0\\xa4\\x9a\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f \\xe0\\xa4\\xac\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\xa4 \\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\xac\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'], shape=(32,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_ds.take(1):\n",
    "    print('i: ',i)\n",
    "    print('j: ', j)\n",
    "    # print('j decoded: ',j.numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3DdjsaS4aqs"
   },
   "source": [
    "# Text Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yZj0w70q0tne"
   },
   "outputs": [],
   "source": [
    "eng_vectorizer = tf.keras.layers.TextVectorization(standardize = tf_lower_and_split_punct_en, output_sequence_length= output_sequence_length)\n",
    "hin_vectorizer = tf.keras.layers.TextVectorization(standardize = tf_lower_and_split_punct_hi, output_sequence_length= output_sequence_length+1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lj-Y5ei_0tmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Adapting to textvectorizer\n",
    "eng_vectorizer.adapt(train_ds.map(lambda x, y: x))\n",
    "hin_vectorizer.adapt(train_ds.map(lambda x, y: y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msq7v_jN0tkT",
    "outputId": "9427b023-0e3f-4b83-9c3f-a7475fa4d20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxtokens:\n",
      "English :  2359\n",
      "Hindi:  3016\n"
     ]
    }
   ],
   "source": [
    "max_token_english = len(eng_vectorizer.get_vocabulary())\n",
    "max_token_hindi = len(hin_vectorizer.get_vocabulary())\n",
    "\n",
    "print('Maxtokens:')\n",
    "print( 'English : ', max_token_english)\n",
    "print('Hindi: ', max_token_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBovvXnn0tjF",
    "outputId": "2b7388d0-77e6-4783-9da7-7c276c5a0c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  उन्होंने मेरी का| मज़ाक उड़ाया\n",
      "\n",
      "Encoded text: ,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(201,), dtype=int64, numpy=\n",
       "array([   2,  173,   40,   20, 1446,  369,    1,    3,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Text: ',some_hindi_text)\n",
    "print('\\nEncoded text: ,')\n",
    "hin_vectorizer(some_hindi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM2zCVr60tgA"
   },
   "source": [
    "## Mapping Vectorizer to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5UvErrnz0teP"
   },
   "outputs": [],
   "source": [
    "def make_vec(x, y ):\n",
    "    x, y = eng_vectorizer(x), hin_vectorizer(y)\n",
    "\n",
    "    y_in = y[:,:-1]\n",
    "    y_out = y[:,1:]\n",
    "    return (x,y_in), y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2TtoPhnZ0tc7"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(make_vec) #.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(make_vec) # .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6BTOBm3i0tbL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   2    8   57 ...    0    0    0]\n",
      " [   2 1244  117 ...    0    0    0]\n",
      " [   2   11  174 ...    0    0    0]\n",
      " ...\n",
      " [   2   12 1015 ...    0    0    0]\n",
      " [   2  152   23 ...    0    0    0]\n",
      " [   2    8  520 ...    0    0    0]], shape=(32, 200), dtype=int64)\n",
      "\n",
      "tf.Tensor(\n",
      "[[   8   57   62 ...    0    0    0]\n",
      " [1244  117 1481 ...    0    0    0]\n",
      " [  11  174   50 ...    0    0    0]\n",
      " ...\n",
      " [  12 1015  457 ...    0    0    0]\n",
      " [ 152   23   26 ...    0    0    0]\n",
      " [   8  520  172 ...    0    0    0]], shape=(32, 200), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for i,j in train_ds.take(1):\n",
    "    print(i[1])\n",
    "    print()\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkUCiddP0tRf"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Tt4J-fOx0tP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units, embedding_dims = 32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "        # self.return_state = return_state\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.LSTM(units, \n",
    "                                       return_sequences = True, \n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        *x, state_h, state_c = self.rnn(x)\n",
    "        tf.print('after encoder return sequence true: ',len(x) )\n",
    "        state = [state_h, state_c]\n",
    "    \n",
    "        return x, state\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts)\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "BsaIPmwK0tOQ"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9mOtgmJw0tMk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, text_processor, units, embedding_dims = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.LSTM(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7vLwbSZi0tK3"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x = self.rnn(x, initial_state=state)\n",
    "    tf.print('decoder output: ', len(x))\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6rWI_DmA0tJB"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CmQFKqTV0tHT"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VtcX4VA00tFx"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(context, next_token, state = state, return_state=True) \n",
    "\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :]/temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QbTvfGlI0tCK",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Translator(tf.keras.Model):\n",
    "#     @classmethod\n",
    "#     def add_method(cls, fun):\n",
    "#         setattr(cls, fun.__name__, fun)\n",
    "#         return fun\n",
    "\n",
    "#     def __init__(self, units, context_text_processor, target_text_processor):\n",
    "#         super().__init__()\n",
    "#         # Build the encoder and decoder\n",
    "#         encoder = Encoder(context_text_processor, units)\n",
    "#         decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "        \n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         context, x = inputs\n",
    "#         tf.print('Before encoder-decoder')\n",
    "#         # tf.print('inputs : ',inputs.shape)\n",
    "#         tf.print('context: ',context.shape)\n",
    "#         tf.print('x      : ',x.shape)\n",
    "#         context = self.encoder(context)\n",
    "#         tf.print()\n",
    "#         logits = self.decoder(context, x)\n",
    "#         tf.print('--'*20)\n",
    "#         tf.print('After encoder-decoder')\n",
    "#         tf.print('context: ',context.shape)\n",
    "#         tf.print('logits : ',logits.shape)\n",
    "#     #TODO(b/250038731): remove this\n",
    "#         try:\n",
    "#           # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "#             del logits._keras_mask\n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator2(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, units, context_text_processor, target_text_processor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_text_processor = context_text_processor\n",
    "        self.target_text_processor = target_text_processor\n",
    "\n",
    "        self.context_vocab_size = context_text_processor.vocabulary_size()\n",
    "        self.target_vocab_size = target_text_processor.vocabulary_size()\n",
    "        \n",
    "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=target_text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=target_text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
    "    \n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding1 = tf.keras.layers.Embedding(self.context_vocab_size, units, mask_zero=True)\n",
    "        self.embedding2 = tf.keras.layers.Embedding(self.target_vocab_size, units, mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.encoder = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='concat',\n",
    "            layer=tf.keras.layers.GRU(units, \n",
    "                                       return_sequences = True, \n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        self.decoder = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='concat',\n",
    "            layer=tf.keras.layers.GRU(units, \n",
    "                                       return_sequences = True, \n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer='glorot_uniform'))\n",
    "        \n",
    "        \n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.target_vocab_size)\n",
    "        \n",
    "        self.encoder_state = None\n",
    "    def call(self, X, y=None):\n",
    "        context, que = X\n",
    "        \n",
    "        #Encoding\n",
    "        # 1. embedding\n",
    "        x = self.embedding1(context)\n",
    "        tf.print('after embedding: x shape: ',x.shape )\n",
    "        x, enc_h, enc_c = self.encoder(x)\n",
    "        self.encoder_state = [enc_h, enc_c]\n",
    "        # tf.print('after encodeing: x shape: ',len(x) )\n",
    "\n",
    "        # encoder_context, encoder_state = x\n",
    "        #Decoding\n",
    "        x = self.embedding1(que)\n",
    "        decoder_context, decoder_state_h, decoder_state_c = self.decoder(x, initial_state=self.encoder_state)\n",
    "        \n",
    "        # tf.print('decoder context shape: ', len(decoder_context))\n",
    "        # logits\n",
    "        logits = self.output_layer(decoder_context)\n",
    "            # tf.print('\\n')\n",
    "        #TODO(b/250038731): remove this\n",
    "        try:\n",
    "          # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "lhj-jdpS0tAI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Translator2(UNITS, eng_vectorizer, hin_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dSxxNTtT0s2z",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample = train_df.sample(5)\n",
    "# some_hindi_text = sample['hindi'].to_numpy()\n",
    "# some_eng_text = sample['english'].to_numpy()\n",
    "\n",
    "# print(some_hindi_text)\n",
    "# print(some_eng_text)\n",
    "\n",
    "# vectorized_eng_text = eng_vectorizer(some_eng_text)\n",
    "# vectorized_eng_text[:,:10]\n",
    "\n",
    "# vectorized_hindi_text = hin_vectorizer(some_hindi_text)\n",
    "# vectorized_hindi_text[:,:10]\n",
    "\n",
    "# vec_hindi_in = vectorized_hindi_text[:,:-1]\n",
    "# vec_hindi_out = vectorized_hindi_text[:,1:]\n",
    "# print(vec_hindi_in[:,:10])\n",
    "# print()\n",
    "# print(vec_hindi_out[:,:10])\n",
    "\n",
    "# encoder = Encoder(eng_vectorizer, UNITS)\n",
    "# # context_eng = encoder((vectorized_eng_text))\n",
    "\n",
    "# new_tx = encoder.convert_input(['hey man'])\n",
    "\n",
    "# # context_eng\n",
    "\n",
    "# # decoder = Decoder(hin_vectorizer, UNITS)\n",
    "# # logits = decoder(context_eng, vec_hindi_in)\n",
    "# # logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DMM5XHX0syX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "wrQnQhGM0stC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IGw3VSmG0sqa"
   },
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "CKPT_DIR = './model_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(CKPT_DIR,  f\"{datetime.now().strftime('%m:%d:%Y, %H:%M:%S')}\"),\n",
    "    monitor= 'loss',\n",
    "    verbose= 0,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    mode= 'auto',\n",
    "    save_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "rg1ZWQV90sn9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "after embedding: x shape:  TensorShape([None, 200, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 12:02:13.678665: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_43/output/_20'\n",
      "2023-02-20 12:02:14.357474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-02-20 12:02:15.947486: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2023-02-20 12:02:15.979367: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_14' defined at (most recent call last):\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/t/.local/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 724, in start\n      self.io_loop.start()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 512, in dispatch_queue\n      await self.process_one()\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 501, in process_one\n      await dispatch(*args)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 408, in dispatch_shell\n      await result\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 731, in execute_request\n      reply_content = await reply_content\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 417, in do_execute\n      res = shell.run_cell(\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_35782/2644086274.py\", line 3, in <module>\n      history = model.fit(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_14'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_14}}]] [Op:__inference_train_function_22574]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tf.executing_eagerly(False)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_14' defined at (most recent call last):\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/t/.local/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 724, in start\n      self.io_loop.start()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 512, in dispatch_queue\n      await self.process_one()\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 501, in process_one\n      await dispatch(*args)\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 408, in dispatch_shell\n      await result\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 731, in execute_request\n      reply_content = await reply_content\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 417, in do_execute\n      res = shell.run_cell(\n    File \"/home/t/.local/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/t/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_35782/2644086274.py\", line 3, in <module>\n      history = model.fit(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/t/miniconda3/envs/tf_new/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_14'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_14}}]] [Op:__inference_train_function_22574]"
     ]
    }
   ],
   "source": [
    "# tf.executing_eagerly(False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch = 50,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3),\n",
    "    model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oAu_jN60slV"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUF3LXpN0sjC"
   },
   "outputs": [],
   "source": [
    ", stateplt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbmAh9W70sgO"
   },
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "  # Process the input texts\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "\n",
    "  # Setup the loop inputs\n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Generate the next token\n",
    "        next_token, done, state = self.decoder.get_next_token(context, next_token, done,  state, temperature)\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Stack the lists of tokens and attention weights.\n",
    "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "    self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDETfmnw0sdp"
   },
   "outputs": [],
   "source": [
    "result = model.translate(['She loves me']) \n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.title('accuracy vs epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPiCALB63AyERfNtjrk0MrL",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
