{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/language-translation-model/blob/main/language_translation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1hJ48ytxM3b"
      },
      "source": [
        "# Language Translation Model (English to Hindi)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrmDJdXC9SfD",
        "outputId": "93f36fac-513c-4459-ecc1-ec3417aa337c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'language-translation-model'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 28 (delta 11), reused 16 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), 419.85 KiB | 3.59 MiB/s, done.\n",
            "/content/language-translation-model\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    \n",
        "    ! git clone https://github.com/tikendraw/language-translation-model.git \n",
        "    os.chdir('language-translation-model') \n",
        "    print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cxy3JvX9Fw4",
        "outputId": "355b7222-6210-4bb9-99c7-569b775308db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTf version:  2.11.0\n",
            "GPU:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# Dependencies\n",
        "! pip install polars -q\n",
        "import polars as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model , optimizers\n",
        "from tensorflow.keras.layers import Attention,GRU, LSTM, Bidirectional, Dense, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, AveragePooling1D, Dropout, concatenate, Concatenate\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# !pip install tensorflow_hub -q\n",
        "# import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print('Tf version: ',tf.__version__)\n",
        "print('GPU: ', is_gpu:=len(tf.config.list_physical_devices('GPU')))\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = './tmp/tfhub'\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
        "None\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "if is_gpu:\n",
        "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "    assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
        "    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(physical_devices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nY49Vs8mxGcD"
      },
      "outputs": [],
      "source": [
        "dataset_url = 'http://www.manythings.org/anki/hin-eng.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H7P06oG8xiS7"
      },
      "outputs": [],
      "source": [
        "# # # Download the dataset\n",
        "# if 'google.colab' in sys.modules:\n",
        "#     # donwload\n",
        "#     !wget $dataset_url -P dataset\n",
        "\n",
        "#     # # Unzip the downloaded file\n",
        "#     !unzip ./dataset/hin-eng.zip -d ./dataset\n",
        "\n",
        "#     # # Show size\n",
        "#     !du -h  ./dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITptJktjzLho"
      },
      "source": [
        "# Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4XP6M5Eb0t5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vraPNhwl0t2Z"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./dataset/hin.txt', sep = '\\t', new_columns = ['english', 'hindi', 'somethingelse'])[['english','hindi']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsHrOgJX0t0m",
        "outputId": "f15cce60-e935-40ee-cb1e-2ff6cc25194d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2908, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "_iYNVAyD0tyx",
        "outputId": "53f47eb1-48c0-4169-ee1e-801186400578"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 2)\n",
              "┌─────────────────────────────────────┬─────────────────────────────────┐\n",
              "│ english                             ┆ hindi                           │\n",
              "│ ---                                 ┆ ---                             │\n",
              "│ str                                 ┆ str                             │\n",
              "╞═════════════════════════════════════╪═════════════════════════════════╡\n",
              "│ Do you believe in God?              ┆ क्या तुम भगवान में विश्वास करते ...   │\n",
              "│ I had my brother repair my bicyc... ┆ मैंने अपने भाई से अपनी साईकल ठीक...  │\n",
              "│ I don't know how, but Tom did it... ┆ पता नहीं कैसे पर टॉम ने कर दिया।    │\n",
              "│ Do you feel any pain in your sto... ┆ क्या आपको पेट में दर्द महसूस हो ...   │\n",
              "│ ...                                 ┆ ...                             │\n",
              "│ According to TV news, there was ... ┆ टीवी समाचार के मुताबिक, भारत में... │\n",
              "│ I will get through with my homew... ┆ मैं उसके आने से पहले अपना होमवर्...    │\n",
              "│ She had a daughter by her first ... ┆ उसकी अपने पहले पति से एक लड़की थ... │\n",
              "│ I am looking for a present for m... ┆ मैं अपनी मम्मी के लिए तोह्फ़ा ढू...    │\n",
              "└─────────────────────────────────────┴─────────────────────────────────┘"
            ],
            "text/html": [
              "<div>\n",
              "<style>\n",
              ".pl-dataframe > thead > tr > th {\n",
              "  text-align: right;\n",
              "}\n",
              "</style>\n",
              "\n",
              "<table border=\"1\" class=\"dataframe pl-dataframe\">\n",
              "<small>shape: (10, 2)</small>\n",
              "<thead>\n",
              "<tr>\n",
              "<th>\n",
              "english\n",
              "</th>\n",
              "<th>\n",
              "hindi\n",
              "</th>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "<td>\n",
              "str\n",
              "</td>\n",
              "</tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;Do you believe...\n",
              "</td>\n",
              "<td>\n",
              "&quot;क्या तुम भगवान...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;I had my broth...\n",
              "</td>\n",
              "<td>\n",
              "&quot;मैंने अपने भाई...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;I don&#x27;t know h...\n",
              "</td>\n",
              "<td>\n",
              "&quot;पता नहीं कैसे ...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;Do you feel an...\n",
              "</td>\n",
              "<td>\n",
              "&quot;क्या आपको पेट ...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;He doesn&#x27;t hav...\n",
              "</td>\n",
              "<td>\n",
              "&quot;उसके पास जीने ...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;How is it goin...\n",
              "</td>\n",
              "<td>\n",
              "&quot;मछली बाज़ार मे...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;According to T...\n",
              "</td>\n",
              "<td>\n",
              "&quot;टीवी समाचार के...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;I will get thr...\n",
              "</td>\n",
              "<td>\n",
              "&quot;मैं उसके आने स...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;She had a daug...\n",
              "</td>\n",
              "<td>\n",
              "&quot;उसकी अपने पहले...\n",
              "</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td>\n",
              "&quot;I am looking f...\n",
              "</td>\n",
              "<td>\n",
              "&quot;मैं अपनी मम्मी...\n",
              "</td>\n",
              "</tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "swlPFpr-0txP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BDxEUwx2jFxB"
      },
      "outputs": [],
      "source": [
        "UNITS = 32\n",
        "EMBEDDING_DIMS = 16\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceSkRUmM0twM"
      },
      "source": [
        "# Prepare the data `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BCw6ZgqV0tuW"
      },
      "outputs": [],
      "source": [
        "# Split the data for train and val\n",
        "train_df, val_df = train_test_split(df, test_size = .02, random_state = 4 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA0bZi7Z0ttP",
        "outputId": "91d3a3bb-f9a6-4322-cdc7-f8983503f0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape:  (2849, 2)\n",
            "val shape:  (59, 2)\n"
          ]
        }
      ],
      "source": [
        "print('train shape: ', train_df.shape)\n",
        "print('val shape: ', val_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z5rNSvh2jFxE"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "puSucMUn0tq-"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_df['english'].to_list(), train_df['hindi'].to_list())).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_df['english'].to_list(), val_df['hindi'].to_list())).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqH0xQMUjFxG",
        "outputId": "0b56f44b-9a7f-4999-b109-a043161e5d1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_APB0Ah64b26"
      },
      "outputs": [],
      "source": [
        "\n",
        "# preprocessing text\n",
        "def tf_lower_and_split_punct_en(text):\n",
        "    # Split accented characters.\n",
        "    # text = tf.text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Keep space, a to z, and select punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    # Add spaces around punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# preprocessing text\n",
        "def tf_lower_and_split_punct_hi(text):\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNyTkJ56pPt",
        "outputId": "25d929ff-67b6-4818-fb29-9652e07a138a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "उन्होंने मेरी का| मज़ाक उड़ाया\n",
            "tf.Tensor(b'[START] \\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\x95\\xe0\\xa4\\xbe |  \\xe0\\xa4\\xae\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\x95 \\xe0\\xa4\\x89\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe [END]', shape=(), dtype=string)\n",
            "[START] उन्होंने मेरी का |  मज़ाक उड़ाया [END]\n"
          ]
        }
      ],
      "source": [
        "some_hindi_text = 'उन्होंने मेरी का| मज़ाक उड़ाया'\n",
        "print(some_hindi_text)\n",
        "b= tf_lower_and_split_punct_hi(some_hindi_text)\n",
        "print(b)\n",
        "print(b.numpy().decode())\n",
        "del(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRZEds9s0tpU",
        "outputId": "22dbaf32-4740-4528-c9c2-40caf9716889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i:  tf.Tensor(\n",
            "[b'He was destined never to meet her again.' b'I meant it as a joke.'\n",
            " b\"I don't know if I'll have time to do it.\" b'I know his address.'\n",
            " b'She reminds me very much of her mother.' b'What is this?'\n",
            " b'We enjoyed skating.' b\"I'd just like to ask Tom one question.\"\n",
            " b'She likes her school a lot.'\n",
            " b'There is no point in studying if you are feeling tired.'\n",
            " b'How do you feel today?' b'They went on talking for hours.'\n",
            " b'He looked back and smiled at me.' b'You made me lose my mind.'\n",
            " b'A rubber ball bounces because it is elastic.'\n",
            " b\"You can't buy that kind of loyalty.\" b\"We're all going to die someday.\"\n",
            " b'Are you the owner of this house?' b'Water is very important.'\n",
            " b'Do you think he will be elected president again?'\n",
            " b'This medicine will take the pain away.'\n",
            " b'Perhaps he will never be famous.'\n",
            " b\"They'll fall in love with each other.\" b\"She isn't married.\"\n",
            " b'Shall I clean the room?' b'The water is not drinkable.'\n",
            " b'These questions are easy to answer.' b'Is your father a teacher?'\n",
            " b\"We're here to protect you.\"\n",
            " b'I brush my teeth at least three times a day.'\n",
            " b'Is it about ten million yen?' b'This lion is very tame.'], shape=(32,), dtype=string)\n",
            "j:  tf.Tensor(\n",
            "[b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb8\\xe0\\xa5\\x80\\xe0\\xa4\\xac \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xab\\xe0\\xa4\\xbf\\xe0\\xa4\\xb0\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa4\\xbf\\xe0\\xa4\\xb2\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x96\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\x86 \\xe0\\xa4\\xa5\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xa4\\xe0\\xa5\\x8b \\xe0\\xa4\\xae\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\x95 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xa4\\xe0\\xa5\\x8c\\xe0\\xa4\\xb0 \\xe0\\xa4\\xaa\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe \\xe0\\xa4\\xa5\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa5\\x87 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xae\\xe0\\xa5\\x87\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb8 \\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f \\xe0\\xa4\\xb8\\xe0\\xa4\\xae\\xe0\\xa4\\xaf \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\x97\\xe0\\xa4\\xbe \\xe0\\xa4\\x95\\xe0\\xa4\\xbf \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa5\\x87 \\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xaa\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa5\\x87 \\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\xae\\xe0\\xa4\\xbe\\xe0\\xa4\\x81 \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa4\\xa6 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\xb2\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88?'\n",
            " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\x95\\xe0\\xa5\\x87\\xe0\\xa4\\x9f\\xe0\\xa4\\xbf\\xe0\\xa4\\x82\\xe0\\xa4\\x97 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe \\xe0\\xa4\\x86\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\x9f\\xe0\\xa5\\x89\\xe0\\xa4\\xae \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x8f\\xe0\\xa4\\x95 \\xe0\\xa4\\xb8\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2 \\xe0\\xa4\\xaa\\xe0\\xa5\\x82\\xe0\\xa4\\x9b\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x9a\\xe0\\xa4\\xbe\\xe0\\xa4\\xb9\\xe0\\xa5\\x82\\xe0\\xa4\\x81\\xe0\\xa4\\x97\\xe0\\xa5\\x80\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x85\\xe0\\xa4\\xaa\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\x95\\xe0\\xa5\\x82\\xe0\\xa4\\xb2 \\xe0\\xa4\\xac\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\xa4 \\xe0\\xa4\\xaa\\xe0\\xa4\\xb8\\xe0\\xa4\\x82\\xe0\\xa4\\xa6 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x85\\xe0\\xa4\\x97\\xe0\\xa4\\xb0 \\xe0\\xa4\\xa5\\xe0\\xa4\\x95 \\xe0\\xa4\\x97\\xe0\\xa4\\x8f \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b \\xe0\\xa4\\xa4\\xe0\\xa5\\x8b \\xe0\\xa4\\xaa\\xe0\\xa4\\xa2\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\x88 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x8b\\xe0\\xa4\\x88 \\xe0\\xa4\\xab\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xa6\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\x86\\xe0\\xa4\\x9c \\xe0\\xa4\\x95\\xe0\\xa5\\x88\\xe0\\xa4\\xb8\\xe0\\xa4\\xbe \\xe0\\xa4\\xb2\\xe0\\xa4\\x97 \\xe0\\xa4\\xb0\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88?'\n",
            " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x94\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa5\\x81\\xe0\\xa4\\x9b \\xe0\\xa4\\x98\\xe0\\xa4\\x82\\xe0\\xa4\\x9f\\xe0\\xa5\\x8b\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f \\xe0\\xa4\\xac\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4\\xe0\\xa4\\x9a\\xe0\\xa5\\x80\\xe0\\xa4\\xa4 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa5\\x80\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xb8\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa5\\x80\\xe0\\xa4\\x9b\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc\\xe0\\xa4\\x95\\xe0\\xa4\\xb0 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa4\\xaa\\xe0\\xa4\\xb0 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\x95\\xe0\\xa5\\x81\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa5\\x81\\xe0\\xa4\\x9d\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\x97\\xe0\\xa4\\xb2 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xb0\\xe0\\xa4\\xac\\xe0\\xa4\\xa1\\xe0\\xa4\\xbc \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\x97\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\xa6 \\xe0\\xa4\\x89\\xe0\\xa4\\x9b\\xe0\\xa4\\xb2\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88 \\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\x95\\xe0\\xa4\\xbf \\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\xb2\\xe0\\xa4\\x9a\\xe0\\xa5\\x80\\xe0\\xa4\\xb2\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x90\\xe0\\xa4\\xb8\\xe0\\xa5\\x80 \\xe0\\xa4\\xb5\\xe0\\xa4\\xab\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\xa6\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\x96\\xe0\\xa4\\xb0\\xe0\\xa5\\x80\\xe0\\xa4\\xa6\\xe0\\xa5\\x80 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe \\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xa4\\xe0\\xa5\\x80\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae \\xe0\\xa4\\xb8\\xe0\\xa4\\xac \\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xb8\\xe0\\xa5\\x80 \\xe0\\xa4\\xa6\\xe0\\xa4\\xbf\\xe0\\xa4\\xa8 \\xe0\\xa4\\xae\\xe0\\xa4\\xb0\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa5\\x87 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x86\\xe0\\xa4\\xaa \\xe0\\xa4\\x87\\xe0\\xa4\\xb8 \\xe0\\xa4\\x98\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xae\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x95 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe?'\n",
            " b'\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\xac\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\xa4 \\xe0\\xa4\\x9c\\xe0\\xa4\\xb0\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xa4\\xe0\\xa5\\x81\\xe0\\xa4\\xae\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\xb2\\xe0\\xa4\\x97\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88 \\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\xab\\xe0\\xa4\\xbf\\xe0\\xa4\\xb0\\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa5\\x87\\xe0\\xa4\\xb8\\xe0\\xa5\\x80\\xe0\\xa4\\xa1\\xe0\\xa5\\x87\\xe0\\xa4\\x82\\xe0\\xa4\\x9f \\xe0\\xa4\\x9a\\xe0\\xa5\\x81\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe\\xe0\\xa4\\x8f\\xe0\\xa4\\x97\\xe0\\xa4\\xbe?'\n",
            " b'\\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\xa6\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\x88 \\xe0\\xa4\\xa6\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa6 \\xe0\\xa4\\x95\\xe0\\xa5\\x8b \\xe0\\xa4\\xa6\\xe0\\xa5\\x82\\xe0\\xa4\\xb0 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0 \\xe0\\xa4\\xa6\\xe0\\xa5\\x87\\xe0\\xa4\\x97\\xe0\\xa5\\x80\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xb8\\xe0\\xa4\\x82\\xe0\\xa4\\xad\\xe0\\xa4\\xb5\\xe0\\xa4\\xa4\\xe0\\xa4\\x83 \\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\xb5\\xe0\\xa4\\xbf\\xe0\\xa4\\x96\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa4\\xa4 \\xe0\\xa4\\x95\\xe0\\xa4\\xad\\xe0\\xa5\\x80 \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x81 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\x8f\\xe0\\xa4\\x97\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x89\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xb9\\xe0\\xa5\\x87\\xe0\\xa4\\x82 \\xe0\\xa4\\x8f\\xe0\\xa4\\x95-\\xe0\\xa4\\xa6\\xe0\\xa5\\x82\\xe0\\xa4\\xb8\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe\\xe0\\xa4\\x8f\\xe0\\xa4\\x97\\xe0\\xa4\\xbe\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xb5\\xe0\\xa4\\xb9 \\xe0\\xa4\\xb6\\xe0\\xa4\\xbe\\xe0\\xa4\\xa6\\xe0\\xa5\\x80\\xe0\\xa4\\xb6\\xe0\\xa5\\x81\\xe0\\xa4\\xa6\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa4\\xae\\xe0\\xa4\\xb0\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\xb8\\xe0\\xa4\\xab\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\x88 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa5\\x82\\xe0\\xa4\\x81 \\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe?'\n",
            " b'\\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa5\\x80 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\xa8\\xe0\\xa4\\xb9\\xe0\\xa5\\x80\\xe0\\xa4\\x82 \\xe0\\xa4\\x9c\\xe0\\xa4\\xbe \\xe0\\xa4\\xb8\\xe0\\xa4\\x95\\xe0\\xa4\\xa4\\xe0\\xa4\\xbe \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x87\\xe0\\xa4\\xa8 \\xe0\\xa4\\xb8\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa5\\x8b\\xe0\\xa4\\x82 \\xe0\\xa4\\x95\\xe0\\xa4\\xbe \\xe0\\xa4\\x9c\\xe0\\xa4\\xb5\\xe0\\xa4\\xbe\\xe0\\xa4\\xac \\xe0\\xa4\\xa6\\xe0\\xa5\\x87\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x86\\xe0\\xa4\\xb8\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe \\xe0\\xa4\\x86\\xe0\\xa4\\xaa\\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe \\xe0\\xa4\\x85\\xe0\\xa4\\xa7\\xe0\\xa5\\x8d\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe\\xe0\\xa4\\xaa\\xe0\\xa4\\x95 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82?'\n",
            " b'\\xe0\\xa4\\xb9\\xe0\\xa4\\xae \\xe0\\xa4\\x86\\xe0\\xa4\\xaa\\xe0\\xa4\\x95\\xe0\\xa5\\x80 \\xe0\\xa4\\xb0\\xe0\\xa4\\x95\\xe0\\xa5\\x8d\\xe0\\xa4\\xb7\\xe0\\xa4\\xbe \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa5\\x87 \\xe0\\xa4\\xb2\\xe0\\xa4\\xbf\\xe0\\xa4\\x8f \\xe0\\xa4\\xaf\\xe0\\xa4\\xb9\\xe0\\xa4\\xbe\\xe0\\xa4\\x81 \\xe0\\xa4\\x86\\xe0\\xa4\\x8f \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\x82\\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xae\\xe0\\xa5\\x88\\xe0\\xa4\\x82 \\xe0\\xa4\\xb0\\xe0\\xa5\\x8b\\xe0\\xa4\\x9c\\xe0\\xa4\\xbc\\xe0\\xa4\\xbe\\xe0\\xa4\\xa8\\xe0\\xa4\\xbe \\xe0\\xa4\\x95\\xe0\\xa4\\xae \\xe0\\xa4\\xb8\\xe0\\xa5\\x87 \\xe0\\xa4\\x95\\xe0\\xa4\\xae \\xe0\\xa4\\xa4\\xe0\\xa5\\x80\\xe0\\xa4\\xa8 \\xe0\\xa4\\xac\\xe0\\xa4\\xbe\\xe0\\xa4\\xb0 \\xe0\\xa4\\xae\\xe0\\xa4\\x82\\xe0\\xa4\\x9c\\xe0\\xa4\\xa8 \\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa5\\x80 \\xe0\\xa4\\xb9\\xe0\\xa5\\x82\\xe0\\xa4\\x81 \\xe0\\xa5\\xa4'\n",
            " b'\\xe0\\xa4\\xa4\\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa5\\x80\\xe0\\xa4\\xac\\xe0\\xa4\\xa8 \\xe0\\xa4\\xa6\\xe0\\xa4\\xb8 \\xe0\\xa4\\x8f\\xe0\\xa4\\x95 \\xe0\\xa4\\x85\\xe0\\xa4\\xb0\\xe0\\xa4\\xac \\xe0\\xa4\\xaf\\xe0\\xa5\\x87\\xe0\\xa4\\xa8 \\xe0\\xa4\\xb9\\xe0\\xa5\\x8b\\xe0\\xa4\\x82\\xe0\\xa4\\x97\\xe0\\xa5\\x87?'\n",
            " b'\\xe0\\xa4\\xaf\\xe0\\xa4\\xb9 \\xe0\\xa4\\xb6\\xe0\\xa5\\x87\\xe0\\xa4\\xb0 \\xe0\\xa4\\xac\\xe0\\xa4\\xb9\\xe0\\xa5\\x81\\xe0\\xa4\\xa4 \\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\xb2\\xe0\\xa4\\xa4\\xe0\\xa5\\x82 \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa5\\xa4'], shape=(32,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for i, j in train_ds.take(1):\n",
        "    print('i: ',i)\n",
        "    print('j: ', j)\n",
        "    # print('j decoded: ',j.numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3DdjsaS4aqs"
      },
      "source": [
        "# Text Vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZoplDkntjFxL"
      },
      "outputs": [],
      "source": [
        "output_sequence_length = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yZj0w70q0tne"
      },
      "outputs": [],
      "source": [
        "eng_vectorizer = tf.keras.layers.TextVectorization(standardize = tf_lower_and_split_punct_en, output_sequence_length= output_sequence_length)\n",
        "hin_vectorizer = tf.keras.layers.TextVectorization(standardize = tf_lower_and_split_punct_hi, output_sequence_length= output_sequence_length+1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj-Y5ei_0tmG",
        "outputId": "efa9aabb-55df-4c1b-afde-86812b0a3170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# Adapting to textvectorizer\n",
        "eng_vectorizer.adapt(train_ds.map(lambda x, y: x))\n",
        "hin_vectorizer.adapt(train_ds.map(lambda x, y: y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msq7v_jN0tkT",
        "outputId": "19dc6f95-89b6-4f87-ad75-99dc27c9d734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maxtokens:\n",
            "English :  2359\n",
            "Hindi:  3016\n"
          ]
        }
      ],
      "source": [
        "max_token_english = len(eng_vectorizer.get_vocabulary())\n",
        "max_token_hindi = len(hin_vectorizer.get_vocabulary())\n",
        "\n",
        "print('Maxtokens:')\n",
        "print( 'English : ', max_token_english)\n",
        "print('Hindi: ', max_token_hindi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBovvXnn0tjF",
        "outputId": "91811dad-1d98-41b3-9791-8b6d97e63e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  उन्होंने मेरी का| मज़ाक उड़ाया\n",
            "\n",
            "Encoded text: ,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(201,), dtype=int64, numpy=\n",
              "array([   2,  173,   40,   20, 1446,  369,    1,    3,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "print('Text: ',some_hindi_text)\n",
        "print('\\nEncoded text: ,')\n",
        "hin_vectorizer(some_hindi_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM2zCVr60tgA"
      },
      "source": [
        "## Mapping Vectorizer to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fTsnsCVwjFxP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5UvErrnz0teP"
      },
      "outputs": [],
      "source": [
        "def make_vec(x, y ):\n",
        "    x, y = eng_vectorizer(x), hin_vectorizer(y)\n",
        "\n",
        "    y_in = y[:,:-1]\n",
        "    y_out = y[:,1:]\n",
        "    return (x,y_in), y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2TtoPhnZ0tc7"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(make_vec) #.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(make_vec) # .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6qd2-dCQjFxR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BTOBm3i0tbL",
        "outputId": "5f3b8aa4-b349-4da7-b2f1-91948069ede7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   2    9    1 ...    0    0    0]\n",
            " [   2   52  122 ...    0    0    0]\n",
            " [   2  591  155 ...    0    0    0]\n",
            " ...\n",
            " [   2    9    1 ...    0    0    0]\n",
            " [   2   12 1406 ...    0    0    0]\n",
            " [   2   12    1 ...    0    0    0]], shape=(32, 200), dtype=int64)\n",
            "\n",
            "tf.Tensor(\n",
            "[[   9    1   22 ...    0    0    0]\n",
            " [  52  122  127 ...    0    0    0]\n",
            " [ 591  155  443 ...    0    0    0]\n",
            " ...\n",
            " [   9    1   42 ...    0    0    0]\n",
            " [  12 1406   89 ...    0    0    0]\n",
            " [  12    1    1 ...    0    0    0]], shape=(32, 200), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for i,j in val_ds.take(1):\n",
        "    print(i[1])\n",
        "    print()\n",
        "    print(j)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkUCiddP0tRf"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUlG5J1jFxT"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Tt4J-fOx0tP5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_processor, units, embedding_dims = 32):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.vocab_size = text_processor.vocabulary_size()\n",
        "        self.units = units\n",
        "        # self.return_state = return_state\n",
        "        # The embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "        # The RNN layer processes those vectors sequentially.\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            merge_mode='sum',\n",
        "            layer=tf.keras.layers.LSTM(units, \n",
        "                                       return_sequences = True, \n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        # 2. The embedding layer looks up the embedding vector for each token.\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 3. The GRU processes the sequence of embeddings.\n",
        "        *x, state_h, state_c = self.rnn(x)\n",
        "        tf.print('after encoder return sequence true: ',len(x) )\n",
        "        state = [state_h, state_c]\n",
        "    \n",
        "        return x, state\n",
        "\n",
        "    def convert_input(self, texts):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "        context = self.text_processor(texts)\n",
        "        context = self(context)\n",
        "        return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_LCPt7qjFxU"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BsaIPmwK0tOQ"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, x, context):\n",
        "\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "        self.last_attention_weights = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PM0TckqjFxV"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9mOtgmJw0tMk"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, text_processor, units, embedding_dims = 32):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.vocab_size = text_processor.vocabulary_size()\n",
        "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "\n",
        "        # 1. The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "        # 2. The RNN keeps track of what's been generated so far.\n",
        "        self.rnn = tf.keras.layers.LSTM(units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = CrossAttention(units)\n",
        "\n",
        "        # 4. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Rix8TVjFxX"
      },
      "source": [
        "### Decoder call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7vLwbSZi0tK3"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "\n",
        "    # 1. Lookup the embeddings\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # 2. Process the target sequence.\n",
        "    x = self.rnn(x, initial_state=state)\n",
        "    tf.print('decoder output: ', len(x))\n",
        "    # 3. Use the RNN output as the query for the attention over the context.\n",
        "    x = self.attention(x, context)\n",
        "    self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "\n",
        "    # Step 4. Generate logit predictions for the next token.\n",
        "    logits = self.output_layer(x)\n",
        "\n",
        "    if return_state:\n",
        "        return logits, state\n",
        "    else:\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6rWI_DmA0tJB"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "    batch_size = tf.shape(context)[0]\n",
        "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "    embedded = self.embedding(start_tokens)\n",
        "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CmQFKqTV0tHT"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "    words = self.id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VtcX4VA00tFx"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "    logits, state = self(context, next_token, state = state, return_state=True) \n",
        "\n",
        "    if temperature == 0.0:\n",
        "        next_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "        logits = logits[:, -1, :]/temperature\n",
        "        next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (next_token == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "    return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QbTvfGlI0tCK",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# class Translator(tf.keras.Model):\n",
        "#     @classmethod\n",
        "#     def add_method(cls, fun):\n",
        "#         setattr(cls, fun.__name__, fun)\n",
        "#         return fun\n",
        "\n",
        "#     def __init__(self, units, context_text_processor, target_text_processor):\n",
        "#         super().__init__()\n",
        "#         # Build the encoder and decoder\n",
        "#         encoder = Encoder(context_text_processor, units)\n",
        "#         decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "        \n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         context, x = inputs\n",
        "#         tf.print('Before encoder-decoder')\n",
        "#         # tf.print('inputs : ',inputs.shape)\n",
        "#         tf.print('context: ',context.shape)\n",
        "#         tf.print('x      : ',x.shape)\n",
        "#         context = self.encoder(context)\n",
        "#         tf.print()\n",
        "#         logits = self.decoder(context, x)\n",
        "#         tf.print('--'*20)\n",
        "#         tf.print('After encoder-decoder')\n",
        "#         tf.print('context: ',context.shape)\n",
        "#         tf.print('logits : ',logits.shape)\n",
        "#     #TODO(b/250038731): remove this\n",
        "#         try:\n",
        "#           # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "#             del logits._keras_mask\n",
        "#         except AttributeError:\n",
        "#             pass\n",
        "\n",
        "#         return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "w0JH_eH3jFxc"
      },
      "outputs": [],
      "source": [
        "class Translator2(tf.keras.Model):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, units, context_text_processor, target_text_processor):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.context_text_processor = context_text_processor\n",
        "        self.target_text_processor = target_text_processor\n",
        "\n",
        "        self.context_vocab_size = context_text_processor.vocabulary_size()\n",
        "        self.target_vocab_size = target_text_processor.vocabulary_size()\n",
        "        \n",
        "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=target_text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=target_text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "    \n",
        "        self.units = units\n",
        "\n",
        "        # The embedding layer converts tokens to vectors\n",
        "        self.embedding1 = tf.keras.layers.Embedding(self.context_vocab_size, units, mask_zero=True)\n",
        "        self.embedding2 = tf.keras.layers.Embedding(self.target_vocab_size, units, mask_zero=True)\n",
        "\n",
        "        # The RNN layer processes those vectors sequentially.\n",
        "        self.encoder = tf.keras.layers.Bidirectional(\n",
        "            merge_mode='concat',\n",
        "            layer=tf.keras.layers.GRU(units, \n",
        "                                       return_sequences = True, \n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "        \n",
        "        self.decoder = GRU(units, \n",
        "                                       return_sequences = True, \n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        \n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "        # 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = CrossAttention(units)\n",
        "        self.last_attention_weights = None\n",
        "        # 4. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.output_layer = tf.keras.layers.Dense(self.target_vocab_size)\n",
        "        \n",
        "        self.encoder_state = None\n",
        "    def call(self, X, y=None):\n",
        "        context, que = X\n",
        "        \n",
        "        #Encoding\n",
        "        # 1. embedding\n",
        "        x = self.embedding1(context)\n",
        "        # tf.print('after embedding: x shape: ',x.shape )\n",
        "        encoder_context, enc_h, enc_c = self.encoder(x)\n",
        "        self.encoder_state = [enc_h, enc_c]\n",
        "        # tf.print('after encodeing: x shape: ',len(x) )\n",
        "\n",
        "        # encoder_context, encoder_state = x\n",
        "        #Decoding\n",
        "        x = self.embedding2(que)\n",
        "        decoder_context, decoder_state_h, decoder_state_c = self.decoder(x, initial_state=self.encoder_state)\n",
        "        \n",
        "        x = self.attention(decoder_context, encoder_context)\n",
        "        # self.last_attention_weights = self.attention.last_attention_weights\n",
        "        # tf.print('decoder context shape: ', len(decoder_context))\n",
        "        # logits\n",
        "        logits = self.output_layer(x)\n",
        "            # tf.print('\\n')\n",
        "        #TODO(b/250038731): remove this\n",
        "        try:\n",
        "          # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbbU_9FJyUtj"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "lhj-jdpS0tAI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = Translator2(UNITS, eng_vectorizer, hin_vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "cellView": "form",
        "id": "dSxxNTtT0s2z",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# sample = train_df.sample(5)\n",
        "# some_hindi_text = sample['hindi'].to_numpy()\n",
        "# some_eng_text = sample['english'].to_numpy()\n",
        "\n",
        "# print(some_hindi_text)\n",
        "# print(some_eng_text)\n",
        "\n",
        "# vectorized_eng_text = eng_vectorizer(some_eng_text)\n",
        "# vectorized_eng_text[:,:10]\n",
        "\n",
        "# vectorized_hindi_text = hin_vectorizer(some_hindi_text)\n",
        "# vectorized_hindi_text[:,:10]\n",
        "\n",
        "# vec_hindi_in = vectorized_hindi_text[:,:-1]\n",
        "# vec_hindi_out = vectorized_hindi_text[:,1:]\n",
        "# print(vec_hindi_in[:,:10])\n",
        "# print()\n",
        "# print(vec_hindi_out[:,:10])\n",
        "\n",
        "# encoder = Encoder(eng_vectorizer, UNITS)\n",
        "# # context_eng = encoder((vectorized_eng_text))\n",
        "\n",
        "# new_tx = encoder.convert_input(['hey man'])\n",
        "\n",
        "# # context_eng\n",
        "\n",
        "# # decoder = Decoder(hin_vectorizer, UNITS)\n",
        "# # logits = decoder(context_eng, vec_hindi_in)\n",
        "# # logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "5DMM5XHX0syX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "wrQnQhGM0stC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "IGw3VSmG0sqa"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "YacbS2vjjFxg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "-i_XW14UjFxg"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "nQ5eDngHjFxh"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "CKPT_DIR = './model_checkpoint'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "mw84m4ZTjFxh"
      },
      "outputs": [],
      "source": [
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(CKPT_DIR,  f\"{datetime.now().strftime('%m:%d:%Y, %H:%M:%S')}\"),\n",
        "    monitor= 'loss',\n",
        "    verbose= 0,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    mode= 'auto',\n",
        "    save_freq='epoch'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "tqtKPvmAjFxi"
      },
      "outputs": [],
      "source": [
        "data_amount = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rg1ZWQV90sn9",
        "outputId": "af17735a-b2df-4f30-db1b-a3ff403d2ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-35b2fec93ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tf.executing_eagerly(False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file73z5x_l6.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mdecoder_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file73z5x_l6.py\", line 15, in tf__call\n        (decoder_context, decoder_state_h, decoder_state_c) = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(x),), dict(initial_state=ag__.ld(self).encoder_state), fscope)\n\n    ValueError: Exception encountered when calling layer 'translator2_1' (type Translator2).\n    \n    in user code:\n    \n        File \"<ipython-input-172-2537c20740c6>\", line 64, in call  *\n            decoder_context, decoder_state_h, decoder_state_c = self.decoder(x, initial_state=self.encoder_state)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py\", line 626, in __call__  **\n            return super().__call__(inputs, **kwargs)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n            raise e.with_traceback(filtered_tb) from None\n        File \"/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py\", line 512, in _validate_state_spec\n            raise validation_error\n    \n        ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, 32), ndim=2), InputSpec(shape=(None, 32), ndim=2)]); however `cell.state_size` is [32]\n    \n    \n    Call arguments received by layer 'translator2_1' (type Translator2):\n      • X=('tf.Tensor(shape=(None, 200), dtype=int64)', 'tf.Tensor(shape=(None, 200), dtype=int64)')\n      • y=None\n"
          ]
        }
      ],
      "source": [
        "# tf.executing_eagerly(False)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch = int(data_amount*(len(train_ds)/EPOCHS)),\n",
        "\n",
        "    validation_data=val_ds.repeat(),\n",
        "    validation_steps = 5,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor = 'masked_loss', patience=3),\n",
        "    model_ckpt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oAu_jN60slV"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "# plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "IXzwfGpMpU3G"
      },
      "outputs": [],
      "source": [
        "class Inference():\n",
        "\n",
        "    def __init__(self,model ):\n",
        "        self.model = model\n",
        "\n",
        "    def convert_input(self, texts):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "        context = self.model.context_text_processor(texts)\n",
        "        context = self.model.embedding1(context)\n",
        "        context, enc_h, enc_c = self.model.encoder(context)\n",
        "        # state = [enc_h, enc_c]\n",
        "        return context\n",
        "\n",
        "    def get_initial_state(self, context):\n",
        "        batch_size = tf.shape(context)[0]\n",
        "        start_tokens = tf.fill([batch_size, 1], self.model.start_token)\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "        embedded = self.model.embedding2(start_tokens)\n",
        "        # context = tf.zeros([batch_size, self.model.units], tf.float32)\n",
        "        context = self.model.decoder.get_initial_state(embedded)[0]\n",
        "        return start_tokens, done, context \n",
        "\n",
        "\n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.model.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "        return result\n",
        "\n",
        "\n",
        "    def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "        logits, state = self.model.decoder(\n",
        "        context, next_token,\n",
        "        state = state,\n",
        "        return_state=True) \n",
        "    \n",
        "        if temperature == 0.0:\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = logits[:, -1, :]/temperature\n",
        "            next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "        # If a sequence produces an `end_token`, set it `done`\n",
        "        done = done | (next_token == self.model.end_token)\n",
        "        # Once a sequence is done it only produces 0-padding.\n",
        "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "        return next_token, done, state\n",
        "\n",
        "\n",
        "    def translate(self,\n",
        "              texts,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "    # Process the input texts\n",
        "        \n",
        "        context = self.convert_input(texts)\n",
        "        print('first: ',context.shape)\n",
        "        batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "        tokens = []\n",
        "        # attention_weights = []\n",
        "        next_token, done, state = self.get_initial_state(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Generate the next token\n",
        "            next_token, done, state = self.get_next_token(context, next_token, done,  state, temperature)\n",
        "\n",
        "            # Collect the generated tokens\n",
        "            tokens.append(next_token)\n",
        "            # attention_weights.append(self.last_attention_weights)\n",
        "\n",
        "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "                break\n",
        "\n",
        "        # Stack the lists of tokens and attention weights.\n",
        "        tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "        # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "        result = self.tokens_to_text(tokens)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "E-FOjbqi6aDM"
      },
      "outputs": [],
      "source": [
        "inf = Inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "XYldFggti6nm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "9529d4a6-3ef3-43d4-e8c6-c660b0e78dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first:  (1, 200, 64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-26ef9f41aa75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'She loves me'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-183-a7f016b56817>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, texts, max_length, temperature)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Generate the next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Collect the generated tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-183-a7f016b56817>\u001b[0m in \u001b[0;36mget_next_token\u001b[0;34m(self, context, next_token, done, state, temperature)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         logits, state = self.model.decoder(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constants\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     def call(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    296\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"gru_3\" is incompatible with the layer: expected shape=(None, None, 32), found shape=(1, 200, 64)"
          ]
        }
      ],
      "source": [
        "ss = inf.translate(['She loves me'])\n",
        "ss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "Utdr6bkrk8Vb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Di7FvIMLpEVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d1f8b1-fc1a-4614-e8a1-a0bf2add1cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 200, 64)\n",
            "(2, 32)\n",
            "(2, 32)\n"
          ]
        }
      ],
      "source": [
        "ss = convert_input([['hello'], ['hwy']])\n",
        "for i in ss:\n",
        "    print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCLmMr_sfW-",
        "outputId": "58ec1352-fa3a-4136-8e7b-c7da9cd4bde5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
              "array([[b'hello how  are you'],\n",
              "       [b'hey'],\n",
              "       [b'who are you']], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "a = [['hello how  are you'],['hey'],[ 'who are you']]\n",
        "aa = tf.convert_to_tensor(a)\n",
        "aa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "XUF3LXpN0sjC",
        "outputId": "037fe6a6-81d6-4aef-a74f-939a8a9ccb81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa03fadde80>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "# plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbmAh9W70sgO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDETfmnw0sdp"
      },
      "outputs": [],
      "source": [
        "result = model.translate(['She loves me']) \n",
        "result[0].numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty6op_uUjFxl"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.title('accuracy vs epochs')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfnUjWD_jFxl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}